{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "battery_multi_bandit.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMGL3h2lrjA5Ry6Va9wUD4Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Itsuki-Hamano123/practice-casualeffect/blob/master/battery_multi_bandit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xte8Vb9aUbCz"
      },
      "source": [
        "# 乾電池の新・旧を見分ける\n",
        "方法：乾電池を垂直に落として、新しい電池（しっかり立つもの）を探す\n",
        "<br>[参考サイト](https://csoption.nifty.com/cs/denki/detail/160701000001/1.htm#:~:text=%E8%AA%BF%E3%81%B9%E3%81%9F%E3%81%84%E4%B9%BE%E9%9B%BB%E6%B1%A0%E3%82%92%E3%83%AA%E3%83%A2%E3%82%B3%E3%83%B3,%E6%8E%A8%E3%81%97%E9%87%8F%E3%82%8B%E3%81%93%E3%81%A8%E3%81%8C%E3%81%A7%E3%81%8D%E3%81%BE%E3%81%99%E3%80%82)\n",
        "-  マイナス面を下に垂直に落としたとき立つ確率は新しい電池ほど高いと仮定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFirAYfgRWmj"
      },
      "source": [
        "import copy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzSoSJH0x36G"
      },
      "source": [
        "# アーム（乾電池）の初期化を行う関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8CUHTv0yCvM"
      },
      "source": [
        "def _initialize_arms(arms_num):\n",
        "    '''指定された本数分のサイズを持つarmsオブジェクトを返す\n",
        "    Returns\n",
        "    ------\n",
        "    arms : list of dict (size: arms_num * dict_key_dim)\n",
        "      2本の場合の例: [{TRIAL_NUM_KEY:0, OBS_REWARD_HISTORY_KEY:[], UCB_REWARD_HISTORY_KEY:[]}, \n",
        "                       {TRIAL_NUM_KEY:0, OBS_REWARD_HISTORY_KEY:[], UCB_REWARD_HISTORY_KEY:[]}]\n",
        "    '''\n",
        "    arm = {TRIAL_NUM_KEY:0, OBS_REWARD_HISTORY_KEY:[], UCB_REWARD_HISTORY_KEY:[]}\n",
        "    arms = [copy.deepcopy(arm) for _ in np.arange(arms_num)]\n",
        "    '''\n",
        "    arms = [{TRIAL_NUM_KEY:0, OBS_REWARD_HISTORY_KEY:[], UCB_REWARD_HISTORY_KEY:[]}, \n",
        "             {TRIAL_NUM_KEY:0, OBS_REWARD_HISTORY_KEY:[], UCB_REWARD_HISTORY_KEY:[]},\n",
        "             {TRIAL_NUM_KEY:0, OBS_REWARD_HISTORY_KEY:[], UCB_REWARD_HISTORY_KEY:[]},\n",
        "             {TRIAL_NUM_KEY:0, OBS_REWARD_HISTORY_KEY:[], UCB_REWARD_HISTORY_KEY:[]},]\n",
        "    '''\n",
        "    return arms"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJPIBz6CViCZ"
      },
      "source": [
        "##報酬を観測する関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBqRhSaaVmYF"
      },
      "source": [
        "def _observe_reward(proba):\n",
        "    '''電池が立ったとき、報酬獲得'''\n",
        "    return 1 if np.random.random() <= proba else 0"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2BUREENWy2S"
      },
      "source": [
        "##観測結果から期待値の予測を計算する関数\n",
        "- UCB-1:Upper Confidence Bound を期待値の更新に使用"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOrSeHLZWQu2"
      },
      "source": [
        "TRIAL_NUM_KEY = \"trial_num\"\n",
        "OBS_REWARD_HISTORY_KEY = \"observed_reward_history\"\n",
        "UCB_REWARD_HISTORY_KEY = \"ucb_reward_hisotry\"\n",
        "\n",
        "\n",
        "def _take_arms_trial_and_recent_reward(arms):\n",
        "    '''各アームの試行回数配列、観測結果配列、期待値のUCB配列を返す\n",
        "\n",
        "    Returns\n",
        "    -----\n",
        "    arms_trial : 各アームの試行回数配列\n",
        "    arms_obs_reward : 各アームの観測結果配列\n",
        "    arms_ucb_reward : 各アームのucb結果配列\n",
        "    '''\n",
        "    arm_num = len(arms)\n",
        "    arms_trial = np.empty(arm_num)\n",
        "    arms_obs_reward = []\n",
        "    arms_ucb_reward = np.empty(arm_num)\n",
        "\n",
        "    for i, arm_info in enumerate(arms):\n",
        "        arm_trial = arm_info[TRIAL_NUM_KEY]\n",
        "        arm_obs_reward = arm_info[OBS_REWARD_HISTORY_KEY]\n",
        "        arm_recent_ucb_reward = arm_info[UCB_REWARD_HISTORY_KEY][-1] if len(arm_info[UCB_REWARD_HISTORY_KEY])!=0 else 0\n",
        "        arms_trial[i] = arm_trial\n",
        "        arms_obs_reward.insert(i, arm_obs_reward)\n",
        "        arms_ucb_reward[i] = arm_recent_ucb_reward\n",
        "    \n",
        "    return arms_trial, arms_obs_reward, arms_ucb_reward\n",
        "\n",
        "\n",
        "def _calc_ucb(arms, total_trial, delta=0.01):\n",
        "    '''期待値の上界を計算\n",
        "    \n",
        "    Parameters\n",
        "    -----\n",
        "    arms : dict\n",
        "    total_trial : 試行回数\n",
        "    delta : 0除算を避ける用途の微小な値\n",
        "\n",
        "    Returns\n",
        "    -----\n",
        "    new_arms_ucb_reward : 各アームの期待値(上界)\n",
        "    '''\n",
        "    arms_trial, arms_obs_reward, _ = _take_arms_trial_and_recent_reward(arms)\n",
        "    \n",
        "    arm_num = len(arms_trial)\n",
        "    new_arms_ucb_reward = np.empty(arm_num)\n",
        "\n",
        "    for i, (trial_num, obs_reward) in enumerate(zip(arms_trial, arms_obs_reward)):\n",
        "        \n",
        "        # 観測結果から期待値の計算\n",
        "        mean_reward = 0 if trial_num == 0 else np.mean(obs_reward)\n",
        "        \n",
        "        # 正則化項の計算\n",
        "        regularization = np.sqrt(2*np.log(total_trial)/(trial_num+delta))\n",
        "\n",
        "        # 期待値と正則化項の情報から期待値の上界を計算\n",
        "        new_reward_ucb = mean_reward + regularization\n",
        "        # 観測された期待値と上界の変化を見たい場合は、コメントアウト外してください \n",
        "        #print('{} + {} = {}'.format(mean_reward, regularization, new_reward_ucb))\n",
        "\n",
        "        new_arms_ucb_reward[i] = new_reward_ucb\n",
        "\n",
        "    return new_arms_ucb_reward\n",
        "\n",
        "\n",
        "def _update_ucb_reward(arms, total_trial):\n",
        "    '''期待値の上界を計算し、結果を挿入\n",
        "    WARNING: 破壊的操作'''\n",
        "    arms_trial, _, arms_ucb_reward = _take_arms_trial_and_recent_reward(arms)\n",
        "    \n",
        "    if arms_trial.sum() != total_trial:\n",
        "        raise Exception(\"現在の試行回数と、各アームの試行回数の合計が一致しません！\")\n",
        "\n",
        "    new_ucb_rewards = _calc_ucb(arms, total_trial, delta=0.01)\n",
        "    \n",
        "    for new_ucb_reward, arm_info in zip(new_ucb_rewards, arms):\n",
        "        # 新しい期待値の上界を挿入\n",
        "        ucb_reward_history = arm_info[UCB_REWARD_HISTORY_KEY]\n",
        "        ucb_reward_history.append(new_ucb_reward)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi2vvoBmrHJq"
      },
      "source": [
        "##電池を1つ選択する関数\n",
        "- 期待値の一番高い電池を選択(UCB-1の方策)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzFiBzhQrbp8"
      },
      "source": [
        "def _choice_argmax_reward_arm(arms):\n",
        "    '''期待値が一番高いarmを選択\n",
        "    選択されたarmの試行回数を1増やす\n",
        "    INFO : 最大値の要素が複数存在する場合、最初に出現する最大値の要素を選択\n",
        "\n",
        "    Returns\n",
        "    -----\n",
        "    choice_index : 選択されたarmのインデックス\n",
        "    choiced_arms : 選択されたarmの試行回数が1加算されたarms\n",
        "    '''\n",
        "    arms_trial, _, arms_ucb_reward = _take_arms_trial_and_recent_reward(arms)\n",
        "    \n",
        "    if arms_trial.sum() == 0:\n",
        "        # 初回実行時はアームをランダムに選択\n",
        "        choice_index = np.random.randint(0, len(arms))\n",
        "    else:\n",
        "        choice_index = np.argmax(arms_ucb_reward)\n",
        "    \n",
        "    choiced_arms = arms.copy()\n",
        "    choiced_arms[choice_index][TRIAL_NUM_KEY] = choiced_arms[choice_index][TRIAL_NUM_KEY] + 1\n",
        "    \n",
        "    return choice_index, choiced_arms    "
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-UrR_Jo2E5N"
      },
      "source": [
        "## 一連の試行を行う関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8PTSnie2NQY"
      },
      "source": [
        "def multi_bandit(arms_num, max_trial, true_probas=None):\n",
        "    '''最適化椀を探索 & 活用する\n",
        "    \n",
        "    Parameters\n",
        "    -----\n",
        "    arms_num : int\n",
        "      アームの本数\n",
        "    max_trial : int\n",
        "      試行回数\n",
        "    true_probas : like ndarray(default:None)\n",
        "      アームが結果を返す真の確率\n",
        "      default値の場合、アームの数だけ0～1の乱数を真の確率として設定\n",
        "\n",
        "    Returns\n",
        "    -----\n",
        "    final_arms_state : dict\n",
        "      全試行が終了した後のアームの状態\n",
        "    choice_history : ndarray(int)\n",
        "      各試行時でのアームの選択結果\n",
        "    true_probas : like ndarry\n",
        "      各アームが報酬を出力する真の確率\n",
        "    '''\n",
        "    if true_probas is None:\n",
        "        true_probas = np.random.random(len(arms_num))\n",
        "    \n",
        "    def _is_size_matched():\n",
        "        return len(true_probas) == arms_num\n",
        "    \n",
        "    if not _is_size_matched():\n",
        "       raise Exception(\"arms_num and true_probas are not match size!\")\n",
        "\n",
        "    arms = _initialize_arms(arms_num)\n",
        "    choice_history = np.ndarray(0)\n",
        "    final_arms_state = copy.deepcopy(arms)\n",
        "    \n",
        "    for i in np.arange(max_trial):\n",
        "        # アームを選択\n",
        "        choice_index, choiced_arms_state = _choice_argmax_reward_arm(final_arms_state)\n",
        "        choice_history = np.append(choice_history, choice_index)\n",
        "\n",
        "        # アーム選択後の報酬の値を観測\n",
        "        obs_reward = _observe_reward(true_probas[choice_index])\n",
        "        observed_arms_state = choiced_arms_state\n",
        "        observed_arms_state[choice_index][OBS_REWARD_HISTORY_KEY].append(obs_reward)\n",
        "\n",
        "        # 観測された報酬の値から、各アームのUCB(期待値の上限値) を算出\n",
        "        _update_ucb_reward(observed_arms_state, i+1)\n",
        "\n",
        "        final_arms_state = observed_arms_state\n",
        "    \n",
        "\n",
        "    def _take_mean_reward():\n",
        "        '''試行で観測された報酬から各アームの期待値を算出して、取り出す\n",
        "        INFO: モデルが各アームの期待値を予測しながらアームの選択を行っているため、\n",
        "              モデルが観測した各アームの平均値と、真の確率が比較可能と考えられる可能性がある\n",
        "        '''\n",
        "        arms_observed_mean_reward = np.ndarray(0)\n",
        "        for i, arm in enumerate(final_arms_state):\n",
        "            arm_observed_mean_reward = np.array(arm[OBS_REWARD_HISTORY_KEY]).mean() if len(arm[OBS_REWARD_HISTORY_KEY]) !=0 else 0\n",
        "            arms_observed_mean_reward = np.append(arms_observed_mean_reward, arm_observed_mean_reward)\n",
        "        \n",
        "        return arms_observed_mean_reward\n",
        "\n",
        "\n",
        "    estimated_probas = _take_mean_reward()\n",
        "\n",
        "    return final_arms_state, choice_history, estimated_probas, true_probas\n"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ-rOc5q9kBo"
      },
      "source": [
        "## バンディットアルゴリズム実行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEn2IiAfyBdX",
        "outputId": "4361427d-e491-4f20-c41a-da21624e4a2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "%%time\n",
        "# 電池の本数\n",
        "battery_num = 4\n",
        "\n",
        "# 電池が垂直に立つ真の確率を設定（今回の予測のキーポイントとなる確率）\n",
        "# マイナス面を下に垂直に落としたとき立つ確率は、新しい電池ほど高い\n",
        "battery_proba = [0.7, 0.5, 0.2, 0.8]\n",
        "\n",
        "# 探索&活用回数\n",
        "trial_num = 50 #@param{type:'number'}\n",
        "\n",
        "\n",
        "# 探索&活用の実行\n",
        "final_arms_state, choice_history, estimated_probas, true_probas = multi_bandit(arms_num=battery_num, max_trial=trial_num,\n",
        "                                                             true_probas=battery_proba)\n",
        "\n",
        "# 結果の配列描画\n",
        "print(\"各電池が立つの真の確率 : {}\".format(true_probas))\n",
        "print(\"各電池が立つ確率の予測値(観測値から期待値を算出) : {}\".format(estimated_probas))\n",
        "print(\"最後らへんに選択されているアームのインデックス : {}\".format(choice_history[-20:]))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "各電池が立つの真の確率 : [0.7, 0.5, 0.2, 0.8]\n",
            "各電池が立つ確率の予測値(観測値から期待値を算出) : [0.6875     0.5        0.16666667 0.77777778]\n",
            "最後らへんに選択されているアームのインデックス : [1. 3. 3. 3. 2. 3. 0. 1. 0. 3. 0. 3. 0. 0. 3. 1. 0. 0. 0. 2.]\n",
            "CPU times: user 8.48 ms, sys: 4.11 ms, total: 12.6 ms\n",
            "Wall time: 14.7 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGpys4YwIQhT"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    }
  ]
}
